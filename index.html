<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset='UTF-8'>
    <title>Action Attention Project</title>
    <link rel="stylesheet" href="styles.css">
    <link href="https://fonts.cdnfonts.com/css/hk-groteks" rel="stylesheet">
        
</head>
<body>
    <div class="title">
        <h1 style="font-size: 40px">Attention and Feature Representation in Spatiotemporal Networks</h1>
    </div>

    <h2  style="font-size: 30px;" class="section-heading">How can we quantify the static and dynamic biases of action recognition models?</h2>
    <div class="column-container">
        <div class="row">
            <div class="column">
               <p style="font-size: 20px;">Models learn features in order to classify videos, but the internal decision-making process is often opaque. Videos have spatial and temporal dimensions and thus exhibit two kinds of features. Both models and datasets can have these feature biases. </p> 
            </div>
            <div class="column">
                <ul class="definitions">
                    <li><em>Static features</em> - Derived from single frames, e.g. texture, color, shape</li>
                    <li><em>Dynamic features</em> -  Derived from multiple frames, e.g motion</li>
                </ul>
            </div>
        </div>
    </div>

    <div class="column-container">
        <div class="row">
            <div class="column">
                <img class ="imgs" src="imgs/fast_000000.gif" alt="Animated GIF">
            </div>
            <div class="column">
                <iframe src="imgs/heatmap.html" height="500" width="500" style="border:none;" title="Iframe Example"></iframe>
            </div>
        </div>
    </div>

    <div class="column-container">
        <div class="row">
            <div class="column">
                <div class="column-text">
                    <h2>Synthetic Dataset</h2>
                    <p style="font-size: 20px;">Real-world videos do not allow for control over exact static or dynamic information; they often contain multiple moving parts, each with its own complex compositions of motions. Hence, we generated a synthetic dataset that allows us to specify the motion and attributes of objects. Each video contains a single ‘target’ object tracing a predefined motion pattern through time, and ‘distractor’ objects in the background. We can therefore use the object’s trajectory as a basis for the dynamic features we expect the model to pick up on. </p>
                </div>
            </div>
            <div class="column">
                <div class="column-text">
                    <h2>Model Visualizations</h2>
                    <p style="font-size: 20px;">To visualize the decision-making process of spatiotemporal models, we use Grad-CAM and a few of its variants, Grad-CAM++ and EigenCAM. GradCAM is a technique that generates coarse localization maps, which highlight the regions of an input that influence a model’s decision-making process. By computing the gradients of a model’s output with respect to the activations of an internal layer, we are able to construct 3D “explanations” of a model’s prediction for any video we pass into it.</p>
            </div>
        </div>
    </div>

    <div class="results">
        <h2>Current Results</h2>
        <p style="font-size:20px;">As demonstrated by the 3D volumes above, the prediction’s localization map does not fully match the target object’s true trajectory. The preliminary results reveal that the model does not need every frame in order to classify a motion with high accuracy; this suggests that the defining dynamic features of our synthetic motions may not extend throughout an entire motion trajectory.</p>
    
    </div>

   <h2 class="section-heading">Authors</h2>


   <div class="container">
    <div class="top-row">
        <div class="author-container">
            <img class ="author-item" src="imgs/headshots/hannah_headshot.jpeg" alt="Photo 1">
            <a href="https://www.linkedin.com/in/hannah-lu-07716918b">Hannah Lu</a>
        
        </div>
        <div class="author-container">
            <img class ="author-item" src="imgs/headshots/headshot.jpeg" alt="Photo 2">
            <a href="https://www.linkedin.com/in/dfonsec/">Dan Fonseca</a>
        </div>
        <div class="author-container">
            <img class ="author-item" src="imgs/headshots/Nikki.jpg" alt="Photo 3">
            <a href="http://www.linkedin.com/in/nicole-falicov">Nicole Falicov</a>
        </div>
    </div>
    <div class="bottom-row">
        <div class="author-container">
            <img class ="author-item" src="imgs/headshots/Rohit.jpg" alt="Photo 4">
            <a href="https://www.linkedin.com/in/rohit-kommuru-b955b71a5/">Rohit Kommuru</a>
        </div>
        <div class="author-container">
            <img class ="author-item" src="imgs/headshots/cwloka.png" alt="Photo 5">
            <a href="https://www.linkedin.com/in/calden-wloka-71281114/">Calden Wloka</a>
        </div>
    </div>
  </div>
  


   <h2 class="section-heading">Further Reading</h2>

   <ul>
    <li><a href="https://arxiv.org/abs/1610.02391">Selvaraju, Ramprasaath R., et al. “Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization.” International Journal of Computer Vision, vol. 128, no. 2, Oct. 2019, pp. 336–59.</a></li>
    <li><a href="https://arxiv.org/abs/1812.03982">Feichtenhofer, Christoph, et al. "SlowFast Networks for Video Recognition." Facebook AI Research (FAIR), 2020</a></li>
    <li><a href="https://arxiv.org/abs/1812.03982">Zhu, Yi, et al. "A Comprehensive Study of Deep Video Action Recognition." Amazon Web Services, 2021</a></li>
  </ul>
    








</body>
</html>