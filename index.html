<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset='UTF-8'>
    <title>Action Attention Project</title>
    <link rel="stylesheet" href="styles.css">
    <link href="https://fonts.cdnfonts.com/css/hk-groteks" rel="stylesheet">
        
</head>
<body>
    <div class="title">
        <h1>Attention and Feature Representation in Spatiotemporal Networks</h1>
    </div>

    <h2 class="section-heading">How can we quantify the static and dynamic biases of action recognition models?</h2>
    <div class="column-container">
        <div class="row">
            <div class="column">
                Lorem ipsum dolor sit amet consectetur adipisicing elit. Consectetur ducimus facilis ad nemo aspernatur. Rerum doloribus odio libero asperiores odit ipsam! Doloremque pariatur ducimus est. Neque, officia delectus? Odio, voluptatibus?
            </div>
            <div class="column">
                Lorem, ipsum dolor sit amet consectetur adipisicing elit. Perferendis inventore atque iusto odit voluptatibus. Dolorem, soluta cumque sint exercitationem delectus voluptatum porro possimus maxime provident voluptate et quae iusto ab?
            </div>
        </div>
    </div>

    

    <h2 class="section-heading">Why quantify static and dynamic biases?</h2>

    <p class="text-content">Models learn features in order to classify videos, but the internal decision-making process is often opaque. Videos have spatial and temporal dimensions, and thus exhibit two kinds of features. Both models and datasets can have these feature biases. 
    </p>

    <div class="img-container">
            <img class ="imgs" src="imgs/fast_000000.gif" alt="Animated GIF">
            <img class="imgs" src="imgs/HeatMap-IMG.png">
    </div>
    
    <div class ="button-container" style="text-align: center;"">
        <button onclick="show()" id="toggle-content" role="button">Show Live Heat Map</button>
    </div>
        <div id="interactive-content">
            <script src="https://code.jquery.com/jquery-3.6.0.min.js"></script>
            <script>
                    $(document).ready(function() {
                    $.ajax({
                        url: 'imgs/fast_000000.html',
                    success: function(data) {
                        $('#interactive-content').html(data);
                    }
                    });
                    document.getElementById('interactive-content').hidden = true
                });
            </script>
        </div>
   
    <script>
       function show() {
            if (document.getElementById('interactive-content').hidden) {
                document.getElementById('interactive-content').hidden = false
                var button = document.getElementById("toggle-content");
                button.innerHTML = "Hide Live Heat Map";
            }
            else {
                document.getElementById('interactive-content').hidden = true
                var button = document.getElementById("toggle-content");
                button.innerHTML = "Show Live Heat Map";
            }
       };
    </script>

<h2 class="section-heading">Are existing action attention recognition models biased towards spatial or temporal features?</h2>

<p class="text-content">Aenean euismod elementum nisi quis eleifend quam adipiscing vitae. Diam donec adipiscing tristique risus nec. Tempor commodo
   ullamcorper a lacus vestibulum sed arcu. Congue quisque egestas diam in arcu cursus euismod quis. Nisl tincidunt eget nullam 
   non. Adipiscing elit pellentesque habitant morbi tristique senectus et netus et. Odio ut enim blandit volutpat maecenas volu
   tpat blandit. Lobortis feugiat vivamus at augue eget arcu dictum varius. Nisl rhoncus mattis rhoncus urna neque. Id leo in v
   itae turpis massa sed elementum tempus.</p>


   <h2 class="section-heading">Authors</h2>


   <div class="container">
    <div class="top-row">
        <div class="author-container">
            <img class ="author-item" src="imgs/headshots/hannah_headshot.jpeg" alt="Photo 1">
            <a href="https://www.linkedin.com/in/hannah-lu-07716918b">Hannah Lu</a>
        
        </div>
        <div class="author-container">
            <img class ="author-item" src="imgs/headshots/headshot.jpeg" alt="Photo 2">
            <a href="https://www.linkedin.com/in/dfonsec/">Dan Fonseca</a>
        </div>
        <div class="author-container">
            <img class ="author-item" src="imgs/headshots/Nikki.jpg" alt="Photo 3">
            <a href="http://www.linkedin.com/in/nicole-falicov">Nicole Falicov</a>
        </div>
    </div>
    <div class="bottom-row">
        <div class="author-container">
            <img class ="author-item" src="imgs/headshots/Rohit.jpg" alt="Photo 4">
            <a href="https://www.linkedin.com/in/rohit-kommuru-b955b71a5/">Rohit Kommuru</a>
        </div>
        <div class="author-container">
            <img class ="author-item" src="imgs/headshots/cwloka.png" alt="Photo 5">
            <a href="https://www.linkedin.com/in/calden-wloka-71281114/">Calden Wloka</a>
        </div>
    </div>
  </div>
  


   <h2 class="section-heading">Further Reading</h2>

   <ul>
    <li><a href="https://arxiv.org/abs/1610.02391">Selvaraju, Ramprasaath R., et al. “Grad-CAM: Visual Explanations from Deep Networks via Gradient-Based Localization.” International Journal of Computer Vision, vol. 128, no. 2, Oct. 2019, pp. 336–59.</a></li>
    <li><a href="https://arxiv.org/abs/1812.03982">Feichtenhofer, Christoph, et al. "SlowFast Networks for Video Recognition." Facebook AI Research (FAIR), 2020</a></li>
    <li><a href="https://arxiv.org/abs/1812.03982">Zhu, Yi, et al. "A Comprehensive Study of Deep Video Action Recognition." Amazon Web Services, 2021</a></li>
  </ul>
    








</body>
</html>